{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "MODEL SIZE IS **2.24 GB** ONLY, BUT EXCELS GREATLY AT ALL NLP TASKS!\n",
        "\n",
        "Choose **Runtime  ->  Change runtime type  ->  T4 GPU (HIT SAVE)**\n",
        "\n",
        "Hit **Ctrl + F9** to run all cells in 1-click."
      ],
      "metadata": {
        "id": "2xClPxhxE4zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "7-rZ_2QEYl9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer\n",
        "import os\n",
        "from threading import Thread\n",
        "\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "#from IPython.display import HTML, display #remove or comment out\n",
        "\n",
        "# Check if running in Colab and set save directory accordingly\n",
        "if IN_COLAB:\n",
        "    drive.mount('/content/drive')\n",
        "    save_directory = '/content/drive/MyDrive/llama_model'\n",
        "else:\n",
        "    save_directory = 'llama_model'  # Or any local path you prefer\n",
        "\n",
        "# Check if the model already exists in the save directory\n",
        "if not os.path.exists(save_directory):\n",
        "    # If the model doesn't exist, download it from Hugging Face\n",
        "    print(\"Model not found in Drive. Downloading from Hugging Face...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\")\n",
        "\n",
        "    # Create the save directory if it doesn't exist\n",
        "    os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "    # Save the tokenizer and model to the save directory\n",
        "    tokenizer.save_pretrained(save_directory)\n",
        "    model.save_pretrained(save_directory)\n",
        "else:\n",
        "    # If the model already exists, load it from the save directory\n",
        "    print(\"Model found in Drive. Loading...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "    model = AutoModelForCausalLM.from_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "rhzdBerxBdQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def prompt(prompt):\n",
        "\n",
        "    display(HTML(\"\"\"\n",
        "    <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;       /* Wrap long lines */\n",
        "        word-wrap: break-word;       /* Break words if necessary */\n",
        "        width: 70% !important; /* Adjust the percentage as needed */\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\"))\n",
        "\n",
        "\n",
        "    model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "    prompt = {prompt}\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Give concise answers!\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "\n",
        "    # Create a streamer object\n",
        "    streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    # Pass the streamer to the generate method\n",
        "    generation_kwargs = dict(\n",
        "        **model_inputs,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=5000,\n",
        "    )\n",
        "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    # Iterate over the generated tokens and print them\n",
        "    for new_text in streamer:\n",
        "        print(new_text, end=\"\", flush=True)\n",
        "\n",
        "    thread.join()"
      ],
      "metadata": {
        "id": "94nFqBJa4ej9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_value():\n",
        "  input_value = input(\"Enter your prompt: \")\n",
        "  print(\"\\nResponse: \")\n",
        "  prompt(input_value)"
      ],
      "metadata": {
        "id": "rz7VeZBb8zJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_value()"
      ],
      "metadata": {
        "id": "3RO0DCAx9YHq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}