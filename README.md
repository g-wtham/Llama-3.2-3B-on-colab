# Llama-3.2-3B-on-colab
Running Llama 3.2 3B 4-bit quantized model (2.04 GB) on Google Colab T4 GPU (free)
